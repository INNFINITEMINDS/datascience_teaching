---
title: "ml_activities"
author: "Greg Medlock"
date: "11/9/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Unsupervised machine learning
# exercise 1
Let's work with a toy dataset to walk through an unsupervised learning algorithm.

```{r cars}
sample1 = c(0,1,1,0,0,0,1,1)
sample2 = c(1,0,0,0,0,0,1,0)
sample3 = c(0,1,1,0,0,1,1,1)
sample4 = c(1,0,0,1,0,0,0,1)
sample5 = c(0,1,1,1,0,1,1,1)
sample6 = c(1,1,0,0,1,0,1,0)
sample7 = c(1,1,1,1,1,0,1,1)
sample_matrix = rbind(sample1,sample2,sample3,sample4,sample5,
                      sample6,sample7)
colnames(sample_matrix) = c('var1','var2','var3','var4','var5','var6','var7','var8')
sample_matrix
```

Given these sample, apply the algorithm we discussed to cluster the data in a hierarchical fashion. To remind you, the algorithm is as follows:

1. Make each sample its own cluster.
2. Find the most similar pair (use the manhattan distance) of clusters and merge them.
3. Continue merging clusters as in step 2. When a cluster has more than one sample, use the maximum similarity between cluster members to determine cluster similarity. If there are ties (e.g. multiple pairs with the same similarity), randomly pick one. We'll see how variability here affects the final result by comparing across groups in the class.
4. When there is a single, high-level cluster that contains all samples, stop clustering.

What does your final clustering look like? Which samples are in your two largest clusters? According to the clustering, which samples are the most similar and most different from each other?

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
